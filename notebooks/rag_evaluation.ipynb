{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EduAI Connect — RAG Pipeline Evaluation\n",
    "Tests 15 questions against the RAG pipeline to verify answer quality.\n",
    "Run this from the project root: `cd ~/Documents/eduai-connect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — same as rag_chain.py\n",
    "import boto3\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "region = \"us-east-1\"\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region)\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_tokens\": 1000}\n",
    ")\n",
    "\n",
    "embeddings = BedrockEmbeddings(client=bedrock_runtime, model_id=\"amazon.titan-embed-text-v1\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an education analytics assistant that helps teachers understand student performance data.\n",
    "Use the context below to answer the question. If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def ask(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return chain.invoke({\"context\": context, \"question\": question, \"chat_history\": \"\"})\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Questions\n",
    "Each question tests a different RAG capability. Check if the answer is correct and relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Specific student lookup\n",
    "# EXPECTED: Should return Jessica Wallace's full profile with grades and attendance\n",
    "print(ask(\"Tell me about Jessica Wallace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Find struggling students\n",
    "# EXPECTED: Should return students with D grades or scores below 70\n",
    "print(ask(\"Which students are failing?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Course-specific query\n",
    "# EXPECTED: Should return students enrolled in Physics with their scores\n",
    "print(ask(\"Who has the lowest grade in Physics?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Attendance query\n",
    "# EXPECTED: Should return students with attendance below 0.80\n",
    "print(ask(\"Which students have poor attendance?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Engagement query\n",
    "# EXPECTED: Should return students with low engagement scores (below 60)\n",
    "print(ask(\"Who has the lowest engagement?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Top performer query\n",
    "# EXPECTED: Should return students with mostly A grades\n",
    "print(ask(\"Who are the top performing students?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Vague query — tests query transformation effectiveness\n",
    "# EXPECTED: Should still find at-risk students despite vague phrasing\n",
    "print(ask(\"Who needs help?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Multi-course comparison\n",
    "# EXPECTED: Should compare Math scores across students\n",
    "print(ask(\"How are students doing in Math?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9: Grade level filter\n",
    "# EXPECTED: Should return 9th graders and their performance\n",
    "print(ask(\"Show me all 9th grade students and their grades\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10: At-risk identification\n",
    "# EXPECTED: Should identify students with combination of low grades + low attendance + low engagement\n",
    "print(ask(\"Which students are at risk of failing based on their grades and attendance?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11: Course with most struggling students\n",
    "# EXPECTED: Should identify which course has the most D/F grades\n",
    "print(ask(\"Which course has the most students struggling?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q12: Specific metric query\n",
    "# EXPECTED: Should list students sorted by engagement score\n",
    "print(ask(\"Rank the students by engagement score from lowest to highest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q13: Actionable recommendation query\n",
    "# EXPECTED: Should provide specific intervention suggestions\n",
    "print(ask(\"What actions should I take to help students who are struggling in Biology?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14: Hallucination test — asking about data that doesn't exist\n",
    "# EXPECTED: Should say it doesn't know — NOT make up an answer\n",
    "print(ask(\"What is the average SAT score for our students?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q15: Hallucination test — asking about a student that doesn't exist\n",
    "# EXPECTED: Should say it doesn't have info on this student\n",
    "print(ask(\"Tell me about John Smith\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "After running all 15 questions, score each answer:\n",
    "- **Correct**: Answer uses real data from the context and is accurate\n",
    "- **Partially Correct**: Answer is on topic but missing details or slightly wrong\n",
    "- **Incorrect**: Answer is wrong or hallucinated\n",
    "- **Good Refusal**: Correctly said \"I don't know\" when data wasn't available (Q14, Q15)\n",
    "\n",
    "| Question | Score | Notes |\n",
    "|----------|-------|-------|\n",
    "| Q1 | | |\n",
    "| Q2 | | |\n",
    "| Q3 | | |\n",
    "| Q4 | | |\n",
    "| Q5 | | |\n",
    "| Q6 | | |\n",
    "| Q7 | | |\n",
    "| Q8 | | |\n",
    "| Q9 | | |\n",
    "| Q10 | | |\n",
    "| Q11 | | |\n",
    "| Q12 | | |\n",
    "| Q13 | | |\n",
    "| Q14 | | |\n",
    "| Q15 | | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
